{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06LlOrdxUIHw",
        "outputId": "2a26c5d6-850a-4000-9d24-d8f642427bad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(('/content/drive'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZYzt4XH7gX3",
        "outputId": "3bc4af41-354c-4f87-8e27-af2c1ad9a86c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JrxJN1OURyp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyX_DiZV6Wqj",
        "outputId": "082e2917-b56b-4db5-d7b7-6947e3e0dcb5"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of CPU cores: 8\n",
            "Is CUDA available: True\n",
            "Number of GPU(s): 1\n",
            "Device Name: Tesla V100-SXM2-16GB\n",
            "Number of Streaming Multiprocessors (SMs): 80\n",
            "device = cuda:0\n",
            "total number of trainable parameters in model: 434944\n",
            "\n",
            "Epoch 1 | Loss: 2.5580 | Accuracy: 0.3101 \n",
            "\t| Bits Per Byte: 3.69 | Time: 20891.91s\n",
            "Validation | Loss: 2.2812 | Accuracy: 0.3583\n",
            "\n",
            "Epoch 2 | Loss: 2.2258 | Accuracy: 0.3688 \n",
            "\t| Bits Per Byte: 3.21 | Time: 20789.24s\n",
            "Validation | Loss: 2.1870 | Accuracy: 0.3755\n",
            "\n",
            "Epoch 3 | Loss: 2.1654 | Accuracy: 0.3798 \n",
            "\t| Bits Per Byte: 3.12 | Time: 21146.95s\n",
            "Validation | Loss: 2.1470 | Accuracy: 0.3831\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_cores = os.cpu_count()\n",
        "print(f\"Number of CPU cores: {num_cores}\")\n",
        "\n",
        "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Number of GPU(s):\", torch.cuda.device_count())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    current_device = torch.cuda.current_device()\n",
        "    gpu_properties = torch.cuda.get_device_properties(current_device)\n",
        "    print(f\"Device Name: {gpu_properties.name}\")\n",
        "    print(f\"Number of Streaming Multiprocessors (SMs): {gpu_properties.multi_processor_count}\")\n",
        "else:\n",
        "    print(\"No GPU available.\")\n",
        "\n",
        "def load_and_process_enwik8(file_path, sequence_length):\n",
        "    # Read data\n",
        "    with open(file_path, 'rb') as f:\n",
        "        data = bytearray(f.read())\n",
        "    data = np.array(data, dtype=np.uint8)\n",
        "\n",
        "    # Split the data into sequences of `sequence_length`\n",
        "    num_sequences = len(data) // sequence_length\n",
        "    data = data[:num_sequences * sequence_length]\n",
        "    data = data.reshape((num_sequences, sequence_length))\n",
        "\n",
        "    # Convert to torch.Tensor\n",
        "    data = torch.tensor(data, dtype=torch.long)\n",
        "\n",
        "    # Create tensor dataset\n",
        "    dataset = TensorDataset(data[:, :-1], data[:, 1:])\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Create sparsity pattern\n",
        "def create_fixed_window_mask(seq_len, window_size, device):\n",
        "    \"\"\"Create a mask for a fixed-length window attention.\"\"\"\n",
        "    indices = torch.arange(seq_len).unsqueeze(0).to(device)\n",
        "    distance = indices - indices.T\n",
        "    mask = (distance.abs() > window_size).float() * -1e9  # Large negative for softmax\n",
        "    return mask\n",
        "\n",
        "def create_blockwise_mask(seq_len, block_size, device):\n",
        "    \"\"\"Create a mask for block-wise attention.\"\"\"\n",
        "    indices = torch.arange(seq_len).unsqueeze(0).to(device) // block_size\n",
        "    mask = (indices != indices.T).float() * -1e9  # Large negative for softmax\n",
        "    return mask\n",
        "\n",
        "class SparseAttention(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, stride):\n",
        "        super(SparseAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.num_heads = num_heads\n",
        "        self.stride = stride\n",
        "        self.head_dim = embed_size // num_heads\n",
        "        assert self.head_dim * num_heads == embed_size, \"Embed size must be divisible by num_heads\"\n",
        "        self.keys = nn.Linear(embed_size, embed_size)\n",
        "        self.queries = nn.Linear(embed_size, embed_size)\n",
        "        self.values = nn.Linear(embed_size, embed_size)\n",
        "        self.feature_projection = nn.Parameter(torch.randn(self.head_dim, self.head_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, E = x.shape\n",
        "        q = self.queries(x).reshape(B, N, self.num_heads, self.head_dim)\n",
        "        k = self.keys(x).reshape(B, N, self.num_heads, self.head_dim)\n",
        "        v = self.values(x).reshape(B, N, self.num_heads, self.head_dim)\n",
        "\n",
        "        q = torch.einsum('bnhe,ei->bnhi', q, self.feature_projection)\n",
        "        k = torch.einsum('bnhe,ei->bnhi', k, self.feature_projection)\n",
        "\n",
        "        # Initialize all scores to negative infinity\n",
        "        scores = torch.full((B, N, self.num_heads, N), float('-inf'), device=x.device)\n",
        "\n",
        "        # Apply scores at strided intervals\n",
        "        stride = self.stride\n",
        "        for i in range(stride):\n",
        "            indices = torch.arange(i, N, stride, device=x.device)\n",
        "            q_strided = q[:, indices, :, :]\n",
        "            k_strided = k[:, indices, :, :]\n",
        "            einsum_result = torch.einsum('bnhd,bmhd->bnhm', q_strided, k_strided) / math.sqrt(self.head_dim)\n",
        "            for j in range(len(indices)):\n",
        "                scores[:, indices[j], :, indices] = einsum_result[:, j, :, :]\n",
        "\n",
        "        attention = F.softmax(scores, dim=-1)\n",
        "        out = torch.einsum('bnhm,bmhe->bnhe', attention, v).reshape(B, N, E)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, block_size):\n",
        "        super().__init__()\n",
        "        self.sparse_attention = SparseAttention(embed_size, num_heads, block_size)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_size, 4 * embed_size),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * embed_size, embed_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = src + self.sparse_attention(self.norm1(src))\n",
        "        src = src + self.ff(self.norm2(src))\n",
        "        return src\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, block_size):\n",
        "        super().__init__()\n",
        "        self.sparse_attention = SparseAttention(embed_size, num_heads, block_size)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.norm3 = nn.LayerNorm(embed_size)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_size, 4 * embed_size),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * embed_size, embed_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, tgt, memory):\n",
        "        if memory.size(0) > tgt.size(0):\n",
        "            memory = memory[:tgt.size(0), :, :]\n",
        "        if tgt.size(1) != memory.size(1):\n",
        "            padding = torch.zeros_like(memory[:, :(tgt.size(1) - memory.size(1)), :])\n",
        "            memory = torch.cat([memory, padding], dim=1)\n",
        "        tgt_attention = self.sparse_attention(self.norm1(tgt))\n",
        "        memory_attention = self.sparse_attention(self.norm2(memory[:, :tgt.size(1), :]))\n",
        "        tgt = tgt + tgt_attention + memory_attention\n",
        "        tgt = tgt + self.ff(self.norm3(tgt))\n",
        "        return tgt\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, num_layers, block_size, num_tokens, max_seq_length):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_tokens, embed_size)\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, max_seq_length, embed_size))\n",
        "        self.enc_layers = nn.ModuleList([TransformerEncoderLayer(embed_size, num_heads, block_size) for _ in range(num_layers)])\n",
        "        self.dec_layers = nn.ModuleList([TransformerDecoderLayer(embed_size, num_heads, block_size) for _ in range(num_layers)])\n",
        "        self.final_layer = nn.Linear(embed_size, num_tokens)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        assert src.size(0) == tgt.size(0), f\"Source and target batch sizes do not match: {src.size(0)} != {tgt.size(0)}\"\n",
        "        src = self.embedding(src) + self.pos_embedding[:,:src.size(1),:]\n",
        "        tgt = self.embedding(tgt) + self.pos_embedding[:,:tgt.size(1),:]\n",
        "        memory = src\n",
        "        for layer in self.enc_layers:\n",
        "            memory = layer(memory)\n",
        "        out = tgt\n",
        "        for layer in self.dec_layers:\n",
        "            out = layer(out, memory)\n",
        "        out = self.final_layer(out)\n",
        "        return out\n",
        "\n",
        "    @property\n",
        "    def total_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "def train(model, criterion, optimizer, data_loader, epoch):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    start_time = time.time()\n",
        "    for batch, (src, tgt) in enumerate(data_loader):\n",
        "        optimizer.zero_grad()\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "        output = model(src, tgt[:, :-1])  # Shifted by one for predicting the next token\n",
        "        loss = criterion(output.view(-1, num_tokens), tgt[:, 1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = output.max(-1)\n",
        "        correct += (predicted == tgt[:, 1:]).view(-1).sum().item()\n",
        "        total += tgt[:, 1:].numel()\n",
        "\n",
        "    # Compute average loss and bits per byte\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = correct / total\n",
        "    end_time = time.time()\n",
        "    epoch_duration = end_time - start_time\n",
        "    bits_per_byte = avg_loss / math.log(2)\n",
        "    print(f'\\nEpoch {epoch} | Loss: {avg_loss:.4f} | Accuracy: {accuracy:.4f} \\n\\t| Bits Per Byte: {bits_per_byte:.2f} | Time: {epoch_duration:.2f}s')\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "def validate(model, criterion, data_loader):\n",
        "    model.eval()\n",
        "    total_loss, total_correct, total_tokens = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in data_loader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            output = model(src, tgt[:, :-1])\n",
        "            loss = criterion(output.view(-1, num_tokens), tgt[:, 1:].reshape(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predictions = torch.max(output, dim=2)\n",
        "            correct = (predictions == tgt[:, 1:]).float().sum()\n",
        "            total_correct += correct.item()\n",
        "            total_tokens += tgt[:, 1:].numel()\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = total_correct / total_tokens\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def plot_metrics(metrics, labels, title, filename):\n",
        "    epochs = range(1, len(metrics[0]) + 1)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for metric, label in zip(metrics, labels):\n",
        "        plt.plot(epochs, metric, label=label)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Value')\n",
        "    plt.legend()\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "sequence_length = 512\n",
        "file_path = './Dl-project/enwik8'\n",
        "\n",
        "# Load and process dataset\n",
        "dataset = load_and_process_enwik8(file_path, sequence_length)\n",
        "\n",
        "# Split dataset into training, validation, and testing\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create DataLoader instances for each set\n",
        "batch_size = 16\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, drop_last=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "# Device Configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Model Parameters\n",
        "embed_size = 64\n",
        "num_heads = 4\n",
        "epochs = 5\n",
        "block_size = 32\n",
        "num_layers = 4\n",
        "num_tokens = 256\n",
        "max_seq_length = sequence_length\n",
        "sequence_length = 256\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device = {device}\")\n",
        "\n",
        "# Instantiate the Model\n",
        "model = Transformer(embed_size, num_heads, num_layers, block_size, num_tokens, max_seq_length).to(device)\n",
        "print(f\"total number of trainable parameters in model: {model.total_parameters}\")\n",
        "model = nn.DataParallel(model, device_ids=[0])\n",
        "\n",
        "# Optimizer and Loss Function\n",
        "optimizer = Adam(model.parameters(), lr=8e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "losses, accuracies, val_losses, val_accuracies = [], [], [], []\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss, train_accuracy = train(model, criterion, optimizer, train_dataloader, epoch)\n",
        "    val_loss, val_accuracy = validate(model, criterion, val_dataloader)\n",
        "    # Save metrics\n",
        "    losses.append(train_loss)\n",
        "    accuracies.append(train_accuracy)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    print(f'Validation | Loss: {val_loss:.4f} | Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "test_loss, test_accuracy = validate(model, criterion, test_dataloader)\n",
        "print(f'Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "plot_metrics([losses, val_losses], ['Training Loss', 'Validation Loss'], 'Training and Validation Loss', 'loss_plot.png')\n",
        "plot_metrics([accuracies, val_accuracies], ['Training Accuracy', 'Validation Accuracy'], 'Training and Validation Accuracy', 'accuracy_plot.png')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}